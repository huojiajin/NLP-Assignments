{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+zOJjZkpKwkVQVJ6OTtcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huojiajin/NLP-Assignments/blob/main/Week2_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the library of textblob\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import the tool of sort\n",
        "from operator import itemgetter\n",
        "# Import the tool of displaying the data\n",
        "import pandas as pd\n",
        "\n",
        "# Import the tool of WordClouds\n",
        "import imageio\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4jyxynkgRv-",
        "outputId": "9d978f4e-57e5-4687-f84b-84790668f5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Romeo & Juliet text\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "h8VF2xhCgr82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNGmdNBuGf10",
        "outputId": "d534ad8b-f7ed-4915-cbde-054ccf3d7764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask_heart.png\tRomeoJuliet.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Romeo & Juliet text.\n",
        "# Retrieve only the first 1,000 characters from the text\n",
        "#\n",
        "textTotal = open('RomeoJuliet.txt').read()\n",
        "blobTotal = TextBlob(textTotal)\n",
        "######################################\n",
        "numChars = 1000\n",
        "text1000 = textTotal[0:numChars+1]\n",
        "blob1000 = TextBlob(text1000)"
      ],
      "metadata": {
        "id": "YlaNd_F8gi-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#1 Count and display the words in the first 1,000 characters of the text. Display all the words by printing 10 words per line"
      ],
      "metadata": {
        "id": "T8oM8OBEhSvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a method to performance the step of word\n",
        "\n",
        "def print_words(list_words, numElementsRow):\n",
        "  for j in range (0, len(list_words), numElementsRow):\n",
        "    for i in range(j, j + numElementsRow):\n",
        "      if (i >= len(list_words)):\n",
        "        break\n",
        "      print(i, '.', list_words[i], end = ',\\t')\n",
        "    print()\n",
        "  print()"
      ],
      "metadata": {
        "id": "qrDIT36whhI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solve the question\n",
        "\n",
        "print_words(blob1000.words, 10)\n",
        "print()\n",
        "print(\"Count of words = \", len(blob1000.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWnHkqLXiwVU",
        "outputId": "4eb06bec-83b2-4f80-d5f3-32d08cc507a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . Project,\t1 . Gutenberg,\t2 . ’,\t3 . s,\t4 . Romeo,\t5 . and,\t6 . Juliet,\t7 . by,\t8 . William,\t9 . Shakespeare,\t\n",
            "10 . This,\t11 . eBook,\t12 . is,\t13 . for,\t14 . the,\t15 . use,\t16 . of,\t17 . anyone,\t18 . anywhere,\t19 . in,\t\n",
            "20 . the,\t21 . United,\t22 . States,\t23 . and,\t24 . most,\t25 . other,\t26 . parts,\t27 . of,\t28 . the,\t29 . world,\t\n",
            "30 . at,\t31 . no,\t32 . cost,\t33 . and,\t34 . with,\t35 . almost,\t36 . no,\t37 . restrictions,\t38 . whatsoever,\t39 . You,\t\n",
            "40 . may,\t41 . copy,\t42 . it,\t43 . give,\t44 . it,\t45 . away,\t46 . or,\t47 . re-use,\t48 . it,\t49 . under,\t\n",
            "50 . the,\t51 . terms,\t52 . of,\t53 . the,\t54 . Project,\t55 . Gutenberg,\t56 . License,\t57 . included,\t58 . with,\t59 . this,\t\n",
            "60 . eBook,\t61 . or,\t62 . online,\t63 . at,\t64 . www.gutenberg.org,\t65 . If,\t66 . you,\t67 . are,\t68 . not,\t69 . located,\t\n",
            "70 . in,\t71 . the,\t72 . United,\t73 . States,\t74 . you,\t75 . ’,\t76 . ll,\t77 . have,\t78 . to,\t79 . check,\t\n",
            "80 . the,\t81 . laws,\t82 . of,\t83 . the,\t84 . country,\t85 . where,\t86 . you,\t87 . are,\t88 . located,\t89 . before,\t\n",
            "90 . using,\t91 . this,\t92 . ebook,\t93 . Title,\t94 . Romeo,\t95 . and,\t96 . Juliet,\t97 . Author,\t98 . William,\t99 . Shakespeare,\t\n",
            "100 . Release,\t101 . Date,\t102 . November,\t103 . 1998,\t104 . Etext,\t105 . 1513,\t106 . Last,\t107 . Updated,\t108 . January,\t109 . 30,\t\n",
            "110 . 2019,\t111 . Language,\t112 . English,\t113 . Character,\t114 . set,\t115 . encoding,\t116 . UTF-8,\t117 . START,\t118 . OF,\t119 . THIS,\t\n",
            "120 . PROJECT,\t121 . GUTENBERG,\t122 . EBOOK,\t123 . ROMEO,\t124 . AND,\t125 . JULIET,\t126 . This,\t127 . etext,\t128 . was,\t129 . produced,\t\n",
            "130 . by,\t131 . the,\t132 . PG,\t133 . Shakespeare,\t134 . Team,\t135 . a,\t136 . team,\t137 . of,\t138 . about,\t139 . twenty,\t\n",
            "140 . Project,\t141 . Gutenberg,\t142 . volunteers,\t143 . THE,\t144 . TRAGEDY,\t145 . OF,\t146 . ROMEO,\t147 . AND,\t148 . JULIET,\t149 . by,\t\n",
            "150 . William,\t151 . Shakespeare,\t152 . Contents,\t153 . THE,\t154 . PROLOGUE,\t155 . ACT,\t156 . I,\t157 . Scene,\t158 . I,\t159 . A,\t\n",
            "160 . public,\t161 . place,\t162 . Scene,\t163 . II,\t164 . A,\t165 . Street,\t166 . Sc,\t\n",
            "\n",
            "\n",
            "Count of words =  167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#2 Count the words in the entire text."
      ],
      "metadata": {
        "id": "4mGhwoUkjwOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Count of words = ', len(blobTotal.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtrDshzoj2Wk",
        "outputId": "bb5490c2-6092-4e45-dead-81b6761712c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of words =  30796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#3 : Count the unique words in the entire text."
      ],
      "metadata": {
        "id": "zaBDcsQHkHnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " print('Total number of unique words in the text = ', len(blobTotal.word_counts.items()))\n",
        " print('Total number of words in the text = ', len(blobTotal.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-sTJcGxkVn9",
        "outputId": "f97a24c5-2d0e-45d0-e58a-f5c073a6ed32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words in the text =  4145\n",
            "Total number of words in the text =  30796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#4 Count the unique words in the entire text after removing the stop-words from the list.\n"
      ],
      "metadata": {
        "id": "tP0aOY7lto8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtain the list of unique words\n",
        "total_unique_words = blobTotal.word_counts.items()\n",
        "print('Total number of unqiue words in the text = ', len(total_unique_words))\n",
        "\n",
        "# Obtain the stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# Declare the result list\n",
        "unique_no_stopwords = []\n",
        "for item in total_unique_words:\n",
        "  if item[0] not in stop_words:\n",
        "    unique_no_stopwords.append(item)\n",
        "\n",
        "print('Total number of unqiue words in the text AFTER removing stop words = ', len(unique_no_stopwords))\n",
        "\n",
        "print('Total number of stop words in the text = ', len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESEHYb0Zumqd",
        "outputId": "1eab4dae-6204-4973-bea1-fefe19e788bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unqiue words in the text =  4145\n",
            "Total number of unqiue words in the text AFTER removing stop words =  4017\n",
            "Total number of stop words in the text =  179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#5 Print the top-10 words in the entire text with highest frequency. Also display words’ frequency"
      ],
      "metadata": {
        "id": "0pNaG0p5ATS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_items = sorted(list(blobTotal.word_counts.items()), key = itemgetter(1), reverse = True)\n",
        "top10Words = sorted_items[0:10]\n",
        "\n",
        "df = pd.DataFrame(top10Words, columns = ['word', 'frequency'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "xKBQnRFqAXLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a9f48d-0a0d-4120-db37-90802f51a37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   word  frequency\n",
            "0   the        876\n",
            "1     ’        869\n",
            "2   and        808\n",
            "3     i        655\n",
            "4    to        626\n",
            "5     a        542\n",
            "6    of        519\n",
            "7    in        395\n",
            "8    is        372\n",
            "9  that        369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#6 Print the top-10 words in the entire text with highest frequency after removing the stop-words from the list. Also display words’ frequency"
      ],
      "metadata": {
        "id": "bS7oJ8fmBiAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_items_no_stopwords = sorted(list(unique_no_stopwords), key = itemgetter(1), reverse = True)\n",
        "top10Words_no_stopwords = sorted_items_no_stopwords[0:10]\n",
        "\n",
        "df2 = pd.DataFrame(top10Words_no_stopwords, columns = ['word', 'frequency'])\n",
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGNfgZrDBoOt",
        "outputId": "c3b02bee-cd14-430b-cfcb-9bfcd1577a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      word  frequency\n",
            "0        ’        869\n",
            "1    romeo        320\n",
            "2     thou        278\n",
            "3   juliet        195\n",
            "4      thy        170\n",
            "5  capulet        163\n",
            "6    nurse        149\n",
            "7     love        148\n",
            "8     thee        138\n",
            "9     lady        117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#7 Count and display the noun-phrases in the first 1,000 characters of the text. Display all the noun-phrases by printing 3 noun-phrases per line."
      ],
      "metadata": {
        "id": "HNpqypeGCgLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_words(blob1000.noun_phrases, 3)\n",
        "print()\n",
        "print(\"Count of noun-phrases = \", len(blob1000.noun_phrases))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ssIcMPCgAw",
        "outputId": "7f991a14-7b01-4fd2-f2e4-3825f05913c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . project gutenberg,\t1 . ’ s,\t2 . romeo,\t\n",
            "3 . juliet,\t4 . william shakespeare,\t5 . restrictions whatsoever,\t\n",
            "6 . project gutenberg license,\t7 . ’ ll,\t8 . title,\t\n",
            "9 . romeo,\t10 . juliet author,\t11 . william shakespeare release date,\t\n",
            "12 . november,\t13 . etext,\t14 . updated,\t\n",
            "15 . january,\t16 . language,\t17 . english character,\t\n",
            "18 . utf-8,\t19 . * * *,\t20 . start of this project gutenberg ebook romeo and juliet,\t\n",
            "21 . * * *,\t22 . pg shakespeare team,\t23 . project gutenberg,\t\n",
            "24 . the tragedy of romeo and juliet,\t25 . william shakespeare contents the prologue,\t26 . act,\t\n",
            "27 . scene,\t28 . public place,\t29 . scene ii,\t\n",
            "30 . sc,\t\n",
            "\n",
            "\n",
            "Count of noun-phrases =  31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#8 Count the noun-phrases in the entire text."
      ],
      "metadata": {
        "id": "gs0snJoUDkC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Count of noun-phrases = ', len(blobTotal.noun_phrases))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E3VX03aDmb1",
        "outputId": "e4af99b5-da74-484f-c8af-034403ebdebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of noun-phrases =  5039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#9 Print the top-10 noun-phrases in the entire text with highest frequency. Also display nounphrases’ frequency."
      ],
      "metadata": {
        "id": "EhxTq4YnD42l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob Dictionary\n",
        "dictionary = blobTotal.np_counts\n",
        "\n",
        "# Standard Python disctionary\n",
        "dictionaryPython = dict(dictionary)\n",
        "\n",
        "#sort\n",
        "rankedList = sorted(dictionary, key = dictionary.get, reverse = True)\n",
        "for i in range(0, 10):\n",
        "  key = rankedList[i]\n",
        "  value = dictionary[key]\n",
        "  print(key + ' repeats ' + str(value) + ' times ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpBedBV4D9iI",
        "outputId": "bfde72f8-3de9-4361-8870-aa96007e5739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "romeo repeats 297 times \n",
            "juliet repeats 177 times \n",
            "nurse repeats 132 times \n",
            "capulet repeats 106 times \n",
            "’ s repeats 98 times \n",
            "mercutio repeats 86 times \n",
            "tybalt repeats 75 times \n",
            "benvolio repeats 72 times \n",
            "friar lawrence repeats 69 times \n",
            "’ ll repeats 68 times \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#10 Print the words in the entire text that contain a string “WA”."
      ],
      "metadata": {
        "id": "23TlZ7KjFdLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in blobTotal.words:\n",
        "  if 'WA' in word:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njuLymywFgai",
        "outputId": "b4e4d82e-4ca1-403a-fba2-1e9ed5a49000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WATCH\n",
            "WATCH\n",
            "WATCH\n",
            "WATCH\n",
            "WATCH\n",
            "WATCH\n",
            "WATCH\n",
            "WATCH\n",
            "WARRANTY\n",
            "WARRANTY\n",
            "WARRANTIES\n",
            "WARRANTIES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question#11 Visualize Word Frequencies. Create WordClouds for the entire text. Use any one of the\n",
        "following 4 masks.\n",
        "a. Heart\n",
        "b. Circle\n",
        "c. Star\n",
        "d. Oval\n",
        "The image files for all the 4 masks are provided in the .png file format."
      ],
      "metadata": {
        "id": "OaKXQ6y_F53s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_image = imageio.imread(str('mask_heart.png'))\n",
        "\n",
        "wordcloud = WordCloud(colormap = 'prism', mask = mask_image, background_color = 'white')\n",
        "wordcloudImage = wordcloud.generate(textTotal)\n",
        "\n",
        "wordcloudFile = wordcloudImage.to_file('Howard.png')\n",
        "wordcloudImage.to_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ly6azGcF-KT",
        "outputId": "2ce09c63-b980-460c-af8f-2bc5202151d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method WordCloud.to_image of <wordcloud.wordcloud.WordCloud object at 0x7feb404780d0>>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_image = imageio.imread(str('mask_circle.png'))\n",
        "\n",
        "wordcloud = WordCloud(colormap = 'prism', mask = mask_image, background_color = 'white')\n",
        "wordcloudImage = wordcloud.generate(textTotal)\n",
        "\n",
        "wordcloudFile = wordcloudImage.to_file('Howard2.png')\n",
        "wordcloudImage.to_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l1LscRRHypI",
        "outputId": "deb854cc-4c9c-469a-f804-f1adefd67f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method WordCloud.to_image of <wordcloud.wordcloud.WordCloud object at 0x7feb40602510>>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}